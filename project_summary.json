{
  "project_name": "OLM Pipeline - Frozen VAE Latent Dynamics",
  "version": "1.0.0",
  "status": "Major Milestone - First Functioning OLM Model",
  "description": "First functioning model on the OLM (Object-Level Manipulation) pipeline that turns raw video into predictive latent state using a frozen VAE and three small LSTMs. This represents a major stepping stone towards completing the OLM project.",

  "core_achievement": {
    "milestone": "First functioning model with the OLM pipeline",
    "significance": "Major stepping stone towards completing OLM project",
    "approach": "Frozen VAE + three-stage LSTM architecture for video state extraction and prediction"
  },

  "architecture": {
    "type": "Video State Extraction Pipeline",
    "components": [
      {
        "name": "Frozen VAE",
        "function": "Stable frame-to-latent encoding/decoding using posterior mean",
        "source": "Stable Diffusion VAE",
        "status": "frozen",
        "scaling": "consistent scaling factor"
      },
      {
        "name": "PatternLSTM",
        "function": "Aggregates short window of latents for pattern detection",
        "parameters": "512 hidden dim, 2 layers, 16 buffer size",
        "status": "frozen",
        "purpose": "temporal aggregation"
      },
      {
        "name": "CompressionLSTM",
        "function": "Compresses PatternLSTM outputs while avoiding variance collapse",
        "parameters": "256 compressed dim, 2 layers",
        "status": "trainable",
        "purpose": "compression"
      },
      {
        "name": "CentralLSTM",
        "function": "Predicts Δz for next step with step controller scaling",
        "parameters": "512 hidden dim, 3 layers",
        "status": "trainable",
        "purpose": "prediction"
      }
    ],
    "design_philosophy": "Clean separation of sensing (frozen VAE), temporal aggregation (Pattern/Compression), and prediction (Central)"
  },

  "performance_metrics": {
    "one_step_prediction": {
      "psnr_db": "25-26",
      "ssim": "~0.99",
      "latent_mse": "2.6e-3 to 3.5e-3",
      "causality": "cos(ẑₜ₊₁, zₜ₊₁) > cos(zₜ, zₜ₊₁) consistently"
    },
    "open_loop_rollouts": {
      "5_step": {
        "psnr_db": "17.6-18.4",
        "ssim": "0.94-0.95"
      },
      "10_step": {
        "psnr_db": "16.1-17.0",
        "ssim": "0.93-0.94"
      },
      "characteristics": "monotonic decay, no blow-ups"
    },
    "stability": {
      "compressor_health": "per-dim std slowly increases, no collapse",
      "central_lstm": "norms bounded with gradient clipping enabled"
    }
  },

  "telemetry_system": {
    "frame_metrics": ["PSNR", "SSIM"],
    "latent_metrics": ["MSE", "cosine similarities", "delta norms"],
    "compression_metrics": ["compressor variance"],
    "rollout_analysis": ["rollout curves"],
    "state_monitoring": ["state norms", "gradient norms"],
    "temporal_analysis": ["scheduled sampling", "teacher forcing ratios"]
  },

  "advantages": {
    "debuggability": "Easier to debug than end-to-end approaches",
    "verification": "Easier to verify no time leakage",
    "iteration_speed": "Fast to iterate on components",
    "state_extraction": "Practical way to extract structured state from environment",
    "latent_properties": "Latent is compact, hashable, and predictable over short horizons"
  },

  "roadmap": {
    "immediate_next": {
      "background_plate_lora": {
        "type": "decoder-side LoRA",
        "rank": "4-8",
        "conditioning": "identity token",
        "training": "supervised by plate frames",
        "inference": "toggle to remove subject",
        "purpose": "world without me functionality"
      }
    },
    "future_enhancements": [
      {
        "name": "ICMI (Identity-Conditioned Matting + Inpainting)",
        "purpose": "Moving cameras and non-static backgrounds",
        "components": ["matting head for subject mask", "latent inpainting with temporal loss"]
      },
      {
        "name": "Subject Replacement",
        "requirements": ["stable removal", "subject-LoRA for target identity", "pose driving from keypoints", "lighting integration", "temporal consistency"]
      },
      {
        "name": "OLM Integration",
        "components": ["novelty driver", "live-VAE adaptation", "verified backbone integration"]
      }
    ]
  },

  "use_cases": [
    "Privacy-preserving streaming",
    "Data collection without operator presence",
    "AR compositing",
    "Controllable training data for downstream policies",
    "Environment state extraction for agents"
  ],

  "technical_details": {
    "processing_rate": "24 FPS",
    "hardware_requirements": ["CUDA-compatible GPU recommended", "webcam or video source"],
    "model_size": "Small, inspectable pipeline",
    "training": "Real-time learning capability",
    "scheduled_sampling": "Teacher forcing with decay",
    "step_controller": "Δz scaling to match observed motion"
  },

  "files_structure": {
    "core_files": [
      {
        "file": "AI.py",
        "purpose": "Main pipeline orchestration, online loop, step controller, metrics emission"
      },
      {
        "file": "vae_processor.py",
        "purpose": "Frozen VAE encode/decode with posterior mean and consistent scaling"
      },
      {
        "file": "lstm_models.py",
        "purpose": "PatternLSTM, CompressionLSTM, CentralLSTM, Δz head, scheduled sampling"
      },
      {
        "file": "camera_gui.py",
        "purpose": "Live view with status and metrics display"
      }
    ],
    "config_files": [
      {
        "file": "ai_config.json",
        "purpose": "LSTM model parameters, training configuration"
      },
      {
        "file": "camera_config.json",
        "purpose": "Camera settings and AI pipeline controls"
      }
    ],
    "output_directories": [
      {
        "directory": "ai_logs/",
        "contents": ["metrics.jsonl", "benchmarks.jsonl"]
      },
      {
        "directory": "vae/",
        "contents": ["VAE model files", "config.json"]
      }
    ]
  },

  "limitations": {
    "horizon": "Short-horizon predictor by design",
    "rollout_degradation": "Rollouts degrade without multi-step objectives",
    "motion_handling": "Δz can overshoot on high-motion frames (mitigated by controller)",
    "throughput": "Modest throughput, emphasis on correctness/telemetry"
  },

  "ethical_considerations": {
    "identity_editing": "Consent required for identity replacement",
    "output_labeling": "Clear labeling for edited outputs required",
    "technical_safeguards": "Small LoRA size to minimize background drift",
    "localization": "Decoder bottleneck + first upsample only"
  },

  "validation_approach": {
    "separation_testing": "Verified clean separation of sensing, compression, and prediction",
    "stability_analysis": "No divergence in open-loop rollouts",
    "causality_verification": "Temporal causality consistently maintained",
    "compression_health": "No variance collapse in compression stage"
  },

  "significance_for_olm": {
    "foundation": "Establishes that OLM's sensing→compression→prediction stack can extract environment state",
    "learning_validation": "Proves short-horizon dynamics learning without large end-to-end training",
    "plugin_architecture": "Stable, debuggable backbone for identity-aware editing",
    "exploration_ready": "Framework ready for novelty-driven exploration integration",
    "adaptation_capable": "Architecture supports live VAE adaptation"
  },

  "reproducibility": {
    "code_availability": "Complete code structure available",
    "training_recipe": "Minimal training recipe ready for LoRA implementation",
    "layer_specifications": "Detailed layer targets for conditioning available",
    "logging_system": "Comprehensive telemetry for verification"
  },
  "experimental_notes": {
    "magnitude_matching_loss_failure": {
      "experiment": "Replace delta clamping with a magnitude-matching loss on latent velocity.",
      "outcome": "Failed. Caused high-frequency noise and gradient spikes.",
      "reason": "The L2 penalty on velocity magnitude forced the model to match large step sizes too quickly. This, combined with the removal of the delta clamping (which acted as a damper), led to an explosion of artifacts as the frozen decoder exaggerated the noise in the latent space.",
      "status": "Reverted. The clamping method was restored for stability."
    },
    "clip_integration_success": {
      "experiment": "Integrate CLIP guidance into main training loop with motion gating and velocity direction loss.",
      "changes": [
        "Folded CLIP loss into main central_loss instead of separate backward/step",
        "Added motion gating: CLIP only applies when delta_ratio < 2.0 OR true_delta_norm below 70th percentile",
        "Used clamped/EMA latent for CLIP computation to align with displayed output",
        "Added velocity direction loss (λ_dir = 0.03) for better alignment without magnitude scaling",
        "Reduced delta_scale_r_max from 3.0 to 2.0 to tame amplitude drift",
        "Enhanced logging with motion status and CLIP application tracking"
      ],
      "outcome": "Successful. CLIP now selectively applies only during calm frames.",
      "benefits": [
        "Prevents semantic nudges during heavy motion periods",
        "Single unified training step eliminates gradient conflicts",
        "Direction loss improves cos_pred_target alignment",
        "Motion gating reduces pastel drift during dynamic scenes",
        "Better logging visibility into when and why CLIP fires"
      ],
      "metrics_improvement": "CLIP now applied=1 only on calm frames (delta_ratio<2.0 or motion below p70), avoiding interference with motion dynamics while maintaining semantic guidance when needed.",
      "status": "Successfully integrated and operating as intended."
    },
    "runtime_levels_system_success": {
      "experiment": "Runtime dimensionality control system with cascading rebuild logic and sweet spot discovery.",
      "implementation": {
        "description": "Live dimension adjustment GUI with staged changes, apply/revert workflow, and minimal rebuild cascade",
        "features": [
          "VAE mode switching (faster_low, balanced, detail_high)",
          "Pattern LSTM hidden dimension (256-1024)",
          "Compression LSTM output dimension (64-1024)",
          "Central LSTM hidden dimension (256-1024)",
          "Prediction horizon slider (1-20 frames, live update)",
          "Staged changes with apply/revert workflow",
          "Cascading rebuild only when necessary",
          "Shape validation with dummy forward passes"
        ]
      },
      "sweet_spot_discovered": {
        "configuration": {
          "vae_mode": "faster_low",
          "vae_latent_dim": 4096,
          "pattern_hidden": 512,
          "compression_out": 128,
          "central_hidden": 768,
          "prediction_horizon": 1
        },
        "rationale": {
          "vae_faster_low": "Forces emphasis on structure/motion over detail, reduces artifact noise from high-detail VAE",
          "pattern_512": "Enough temporal capacity for short-to-mid dependencies without hallucination",
          "compression_128": "Aggressive simplification acts as concept filter, strips fine texture, preserves dominant shapes/motion",
          "central_768": "Larger than baseline 512, juggles more dynamics across frames without exploding compute",
          "horizon_1": "Locks to immediate next-frame clarity, explains sharp YouTube scene performance"
        },
        "metrics_at_sweet_spot": {
          "mse": "~0.024 (tight alignment)",
          "psnr_db": "~16 (decent given compression)",
          "ssim": "~0.83 (high structural similarity)"
        },
        "interpretation": "Pipeline tuned to emphasize scene stability over pixel fidelity. VAE + compression prioritize structural coherence."
      },
      "horizon_stability_curve_discovery": {
        "description": "Discovered natural stability range of architecture through horizon experimentation",
        "ranges": {
          "horizon_1_2": {
            "behavior": "Tight lock, clean and structurally accurate",
            "fidelity": "high",
            "abstraction": "low"
          },
          "horizon_3_5": {
            "behavior": "Dreamlike center effect - extrapolates concept persistence rather than literal pixels",
            "fidelity": "medium",
            "abstraction": "high",
            "description": "Model leans on temporal abstraction, creates conceptual continuity"
          },
          "horizon_8_plus": {
            "behavior": "Collapse/noise - signal drift without correction",
            "fidelity": "low",
            "abstraction": "breakdown",
            "reason": "Weights not updating fast enough to stabilize long rollouts, latent collapse kicks in"
          }
        },
        "concept_fidelity_curve": {
          "low_horizon": "high fidelity, low imagination",
          "mid_horizon": "lower fidelity, high abstraction (dreamlike)",
          "high_horizon": "system breakdown"
        },
        "value": "Discovered how far LSTM stack can reliably stretch before latent collapse. Baseline curve for temporal abstraction support."
      },
      "future_testing_plan": {
        "approach": "3×3 grid testing: horizon × compression_size × central_hidden",
        "purpose": "Systematically map stability region boundaries",
        "baseline": "Sweet spot config serves as baseline clarity mode",
        "lever_for_control": "Horizon becomes knob for hallucination vs. stability tradeoff",
        "metric_tracking": "Log MSE, SSIM, novelty scores across horizons to create baseline curve",
        "future_comparison": "When adding ROI vision or richer levers, compare shifts in collapse point (e.g., 10→20 frames = extended temporal reasoning)"
      },
      "outcome": "Major success. System enables runtime architectural experimentation and discovered optimal configuration.",
      "benefits": [
        "No restart required for dimension changes",
        "Instant feedback on stability vs. abstraction tradeoffs",
        "Discovered natural operational sweet spot empirically",
        "Mapped temporal reasoning limits of architecture",
        "Created baseline for future enhancement comparison",
        "Horizon lever enables controlled hallucination for creative applications"
      ],
      "status": "Successfully deployed and validated. Sweet spot configuration delivers optimal balance of clarity, stability, and computational efficiency."
    }
  }
}